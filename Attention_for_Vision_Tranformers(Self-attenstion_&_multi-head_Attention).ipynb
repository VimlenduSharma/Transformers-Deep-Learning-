{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92d3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using_PyTorch\n",
    "import os\n",
    "import copy \n",
    "import math\n",
    "import typing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c66e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ea73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoneFloat=typing.Union[None, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defcd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention_in_General\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim:int, chan:int, num_heads:int=1, qkv_bias:bool=False, qk_scale:NoneFloat=None):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.chan=chan\n",
    "        self.head_dim=self.chan//self.num_heads\n",
    "        self.scale=qk_scale or self.head_dim**-0.5\n",
    "        assert self.chan % self.num_heads==0\n",
    "        \n",
    "        #Define_layers\n",
    "        self.qkv=nn.Linear(dim, chan*3, bias=qkv_bias)\n",
    "        self.proj=nn.Linear(chan, chan)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C=x.shape #(Dim:(Batch, num_tokens, token_len))\n",
    "        qkv=self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v=qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        #calculate_attention\n",
    "        attn=(q*self.scale) @ k.transpose(-2, -1)\n",
    "        attn=attn.softmax(dim=-1)\n",
    "        \n",
    "        x=(attn @ v).transpose(1, 2).reshape(B, N, self.chan)\n",
    "        x=self.proj(x)\n",
    "        v=v.transpose(1, 2).reshape(B, N, self.chan)\n",
    "        x=v+x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c7eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\ttoken size: 49\n"
     ]
    }
   ],
   "source": [
    "#Single_headed_Attention\n",
    "#Define_an_input\n",
    "token_len=7*7\n",
    "channels=64\n",
    "num_tokens=100\n",
    "batch=13\n",
    "x=torch.rand(batch, num_tokens, token_len)\n",
    "B, N, C=x.shape\n",
    "print('Input Dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n",
    "\n",
    "A=Attention(dim=token_len, chan=channels, num_heads=1, qkv_bias=False, qk_scale=None)\n",
    "A.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for queries are\n",
      "\tbatchsize: 13 \n",
      "\u0007ttention heads: 1 \n",
      "\n",
      "umber of tokens: 100 \n",
      "\n",
      "umber of tokens: 64\n",
      "See that the dimensions for queries, keys & values are same:\n",
      "\tShape of Q: torch.Size([13, 1, 100, 64]) \n",
      "\tshape of k: torch.Size([13, 1, 100, 64]) \n",
      "\tshape of v: torch.Size([13, 1, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "qkv=A.qkv(x).reshape(B, N, 3, A.num_heads, A.head_dim).permute(2, 0, 3, 1, 4)\n",
    "q, k, v=qkv[0], qkv[1], qkv[2]\n",
    "print('Dimensions for queries are\\n\\tbatchsize:', q.shape[0], '\\n\\attention heads:', q.shape[1], '\\n\\number of tokens:', q.shape[2], '\\n\\number of tokens:', q.shape[3])\n",
    "print('See that the dimensions for queries, keys & values are same:')\n",
    "print('\\tShape of Q:', q.shape, '\\n\\tshape of k:', k.shape, '\\n\\tshape of v:', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a220243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of attention are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens 100\n"
     ]
    }
   ],
   "source": [
    "attn=(q*A.scale) @ k.transpose(-2, -1)\n",
    "print('Dimensions of attention are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens', attn.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26949f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for atten are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 100\n"
     ]
    }
   ],
   "source": [
    "#calculate_softmax_of_A_which_does_not_change_its_shape\n",
    "attn=attn.softmax(dim=-1)\n",
    "print('Dimensions for atten are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ac3d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of x are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 1 \n",
      "\tnumber of tokens 100 \n",
      "\tlength of tokens 64\n"
     ]
    }
   ],
   "source": [
    "x=attn @ v\n",
    "print('Dimensions of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens', x.shape[2], '\\n\\tlength of tokens', x.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a651e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "#output_x_is_reshaped\n",
    "x=x.transpose(1, 2).reshape(B, N, A.chan)\n",
    "print('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e87965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of x are\n",
      "\tbatch size: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "x=A.proj(x)\n",
    "print('Dimensions of x are\\n\\tbatch size:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe7bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of input x: (13, 100, 49)\n",
      "Current shape of x: (13, 100, 64)\n",
      "Shape of V: (13, 100, 64)\n",
      "After Skip connections, dimensions of x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "orig_shape=(batch, num_tokens, token_len)\n",
    "curr_shape=(x.shape[0], x.shape[1], x.shape[2])\n",
    "v=v.transpose(1, 2).reshape(B, N, A.chan)\n",
    "v_shape=(v.shape[0], v.shape[1], v.shape[2])\n",
    "print('Original shape of input x:',orig_shape)\n",
    "print('Current shape of x:', curr_shape)\n",
    "print('Shape of V:', v_shape)\n",
    "x=v+x\n",
    "print('After Skip connections, dimensions of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dfe2628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimensions are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\ttoken size: 49\n"
     ]
    }
   ],
   "source": [
    "#Multi-headed Self Attention\n",
    "#use_4_attention_heads\n",
    "#define_input\n",
    "token_len=7*7\n",
    "channels=64\n",
    "num_tokens=100\n",
    "batch=13\n",
    "num_heads=4\n",
    "x=torch.rand(batch, num_tokens, token_len)\n",
    "B, N, C=x.shape\n",
    "print('Input Dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n",
    "#Define_the-Module\n",
    "MSA=Attention(dim=token_len, chan=channels, num_heads=num_heads, qkv_bias=False, qk_scale=None)\n",
    "MSA.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b96ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head Dimension=chan/num_heads= 64 / 4 = 16\n",
      "Dimensions for Queries are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tnew length of tokens: 16\n",
      "See that the dimensions for queries, keys and values are all the same:\n",
      "\tshape of Q: torch.Size([13, 4, 100, 16]) \n",
      "\tshape of K: torch.Size([13, 4, 100, 16]) \n",
      "\tshape of V: torch.Size([13, 4, 100, 16])\n"
     ]
    }
   ],
   "source": [
    "qkv=MSA.qkv(x).reshape(B, N, 3, MSA.num_heads, MSA.head_dim).permute(2, 0, 3, 1, 4)\n",
    "q, k, v=qkv[0], qkv[1], qkv[2]\n",
    "print('Head Dimension=chan/num_heads=', MSA.chan, '/', MSA.num_heads, '=', MSA.head_dim)\n",
    "print('Dimensions for Queries are\\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\n",
    "print('See that the dimensions for queries, keys and values are all the same:')\n",
    "print('\\tshape of Q:', q.shape, '\\n\\tshape of K:', k.shape, '\\n\\tshape of V:', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283c70b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of attention are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 100\n"
     ]
    }
   ],
   "source": [
    "#Query-Key_multiplication\n",
    "attn=(q*MSA.scale)@k.transpose(-2, -1)\n",
    "print('Dimension of attention are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f64bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x are\n",
      "\tbatchsize: 13 \n",
      "\tattention heads: 4 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "attn=attn.softmax(dim=-1)\n",
    "x=attn@v\n",
    "print('Dimension of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9129dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DImensions of x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "#concatenate_all_xi's\n",
    "x=x.transpose(1, 2).reshape(B, N, MSA.chan)\n",
    "print('DImensions of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a143b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tnumber of tokens: 64\n",
      "Original Shape of input x: (13, 100, 49)\n",
      "Current Shape of x: (13, 100, 64)\n",
      "Shape of V: (13, 100, 64)\n",
      "After Skip connections, dimensions of x are\n",
      "\tbatchsize: 13 \n",
      "\tnumber of tokens: 100 \n",
      "\tlength of tokens: 64\n"
     ]
    }
   ],
   "source": [
    "x=MSA.proj(x)\n",
    "print('Dimension of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2])\n",
    "orig_shape=(batch, num_tokens, token_len)\n",
    "curr_shape=(x.shape[0], x.shape[1], x.shape[2])\n",
    "v=v.transpose(1, 2).reshape(B, N, A.chan)\n",
    "v_shape=(v.shape[0], v.shape[1], v.shape[2])\n",
    "print('Original Shape of input x:', orig_shape)\n",
    "print('Current Shape of x:', curr_shape)\n",
    "print('Shape of V:', v_shape)\n",
    "x=v+x\n",
    "print('After Skip connections, dimensions of x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
