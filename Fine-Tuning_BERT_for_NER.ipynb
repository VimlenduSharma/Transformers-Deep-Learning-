{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f114ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import_libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, AdamW, BertForTokenClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1935dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define_entity_types_and_number_of_labels\n",
    "entity_types = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "num_labels = len(entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dd2a933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained_BERT_tokenizer_and_model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ad2f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define_training_parameters\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e1dcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_sample = [\n",
    "    {\"text\": \"John Works at Google in New York.\", \"labels\": {\"entities\": [(0, 4, \"PER\"), (17, 23, \"ORG\"), (27, 35, \"LOC\")]}},\n",
    "    {\"text\": \"Apple Inc. is a technology company.\", \"labels\": {\"entities\": [(0, 10, \"ORG\")]}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d2f0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format_data(dataset, tokenizer, entity_types):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"labels\"][\"entities\"]\n",
    "        \n",
    "        # Tokenize_the_text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Initialize_labels_for_each_token_as_'O'_(outside_of_any_entity)\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # Update_labels_for_entity_spans\n",
    "        for start, end, entity_type in entities:\n",
    "            # Tokenize_the_prefix_to_get_correct_offset\n",
    "            prefix_tokens = tokenizer.tokenize(text[:start])\n",
    "            start_token = len(prefix_tokens)\n",
    "            \n",
    "            # Tokenize the_entity_to_get_its_length\n",
    "            entity_tokens = tokenizer.tokenize(text[start:end])\n",
    "            end_token = start_token + len(entity_tokens) - 1\n",
    "            \n",
    "            # Ensure start_token and end_token are within bounds\n",
    "            if start_token < len(tokens):\n",
    "                labels[start_token] = f\"B-{entity_type}\"\n",
    "                for i in range(start_token + 1, min(end_token + 1, len(tokens))):\n",
    "                    labels[i] = f\"I-{entity_type}\"\n",
    "        \n",
    "        # Convert tokens to input IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Convert labels to label IDs\n",
    "        label_ids = [entity_types.index(label) if label in entity_types else entity_types.index('O') for label in labels]\n",
    "        \n",
    "        # Padding for input IDs and labels to match the model's maximum length\n",
    "        padding_length = tokenizer.model_max_length - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        label_ids += [entity_types.index('O')] * padding_length\n",
    "        \n",
    "        tokenized_data.append({'input_ids': input_ids, 'labels': label_ids})\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids_tensor = torch.tensor([item['input_ids'] for item in tokenized_data])\n",
    "    label_ids_tensor = torch.tensor([item['labels'] for item in tokenized_data])\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(input_ids_tensor, label_ids_tensor)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be0eb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data = tokenize_and_format_data(train_dataset_sample, tokenizer, entity_types)\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "953a7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|                                         | 0/1 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Epoch 1/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 1: 2.348078966140747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 2: 0.5442062616348267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 3: 0.16310027241706848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 4: 0.13836759328842163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 5: 0.07499676942825317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 6: 0.06419733166694641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 7: 0.05740748718380928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 8: 0.05477806553244591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|█████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 9: 0.05383329838514328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 10: 0.053104326128959656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 11: 0.0510014146566391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 12: 0.04861518740653992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 13: 0.04677664116024971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 14: 0.04556713253259659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 15: 0.04445786029100418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_input_ids, batch_labels = batch\n",
    "        batch_input_ids = batch_input_ids.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        batch_labels = batch_labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch_input_ids, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    print(f\"Average loss for epoch {epoch+1}: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13066ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.9386,  0.4486, -0.0478, -0.0886, -0.7108, -0.1851,  0.2494],\n",
      "         [ 0.3358, -0.5510,  0.2315, -0.3240, -0.2289, -0.5868,  0.2912],\n",
      "         [ 1.3937, -0.6088, -0.0767, -0.4081, -0.5356,  0.4041, -0.2571],\n",
      "         [ 0.8568, -0.5871, -0.1098, -0.3460, -0.2877,  0.3004, -0.4386],\n",
      "         [ 0.8510, -0.4462,  0.1478, -0.2943, -0.5911,  0.0192, -0.2116],\n",
      "         [ 1.4181,  0.3901, -0.2994, -0.0149, -0.5432,  0.5404, -0.2110],\n",
      "         [ 1.5397, -0.2353, -0.5442, -0.0059, -0.3951, -0.1309, -0.9379],\n",
      "         [ 1.3004, -0.2631, -0.5439, -0.1184, -0.4259,  0.6136, -0.4864],\n",
      "         [ 1.7083, -0.7097, -0.0932, -0.2090,  0.2152,  0.2975, -0.2829],\n",
      "         [ 1.6492, -0.6368, -0.3893, -0.7716, -0.9038, -0.0788, -0.4138],\n",
      "         [ 1.1298, -0.6394,  0.3592, -0.2730, -0.5218,  0.2692, -0.0870],\n",
      "         [ 0.4108, -0.0980,  0.0254, -0.0092, -0.6833, -0.6284, -0.1557],\n",
      "         [ 1.2724,  0.2668,  0.1499, -0.2527, -0.6540, -0.4585, -0.3240]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "text_medical=\"The patient was prescribed aspirin for pain relief.\"\n",
    "#tokenizing_the_text\n",
    "inputs_medical=tokenizer(text_medical, return_tensors=\"pt\")\n",
    "outputs_medical=model(**inputs_medical)\n",
    "print(outputs_medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a57c0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Entities: ['[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]']\n"
     ]
    }
   ],
   "source": [
    "#extracting_predicted_labels\n",
    "predicted_labels_medical=outputs_medical.logits.argmax(dim=-1)\n",
    "#mapping_labels_to_entity_names\n",
    "entities_medical=[tokenizer.decode(token) for token in predicted_labels_medical[0]]\n",
    "print(\"Medical Entities:\", entities_medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62468f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Entities: ['[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]', '[ P A D ]']\n"
     ]
    }
   ],
   "source": [
    "text_legal=\"This aggrement is entered into on this 1st day of January, 2023, between Company X and Company Y.\"\n",
    "inputs_legal=tokenizer(text_legal, return_tensors=\"pt\")\n",
    "outputs_legal_1=model(**inputs_legal)\n",
    "predicted_labels_legal=outputs_legal_1.logits.argmax(dim=-1)\n",
    "entities_legal=[tokenizer.decode(token) for token in predicted_labels_legal[0]]\n",
    "print(\"Legal Entities:\", entities_legal)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
